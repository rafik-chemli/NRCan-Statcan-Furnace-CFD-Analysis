{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafik-chemli/NRCan-Statcan-Furnace-CFD-Analysis/blob/main/NRCan_Statcan_CFD_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2tuMsUTZqLp",
        "outputId": "85ad16e3-93f1-4a0a-c663-f066896ac4c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # Ignore If not using google drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdVPOTN-FqtE"
      },
      "source": [
        "## PreProcessing\n",
        "This code processes multiple CSV files containing particle information in a target folder, calculates relevant physical quantities, determines the state of particles based on these calculations, and aggregates the results in a static format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zg12FAKZqxD"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Takes ~ 25mins for 10 files of ~1go\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/statcan/furnace/CFD_particle_data'\n",
        "rho_L = 3440\n",
        "gamma = 0.568\n",
        "\n",
        "def sanitize_column_names(dataframe):\n",
        "    \"\"\"\n",
        "    Sanitize column names by replacing spaces, periods, slashes, and dashes with underscores.\n",
        "    :param dataframe: DataFrame with column names to sanitize.\n",
        "    :return: DataFrame with sanitized column names.\n",
        "    \"\"\"\n",
        "    sanitized_columns = dataframe.columns.str.strip().str.replace(' ', '_').str.replace('.', '').str.replace('/', '_').str.replace('-', '')\n",
        "    dataframe.columns = sanitized_columns\n",
        "    return dataframe\n",
        "\n",
        "def calculate_we_bo_lambda(row, rho_L, gamma):\n",
        "    \"\"\"\n",
        "    Calculate Weber number, Bond number, and lambda squared for a particle.\n",
        "    :param row: Series representing a particle.\n",
        "    :param rho_L: Liquid density.\n",
        "    :param gamma: Surface tension coefficient.\n",
        "    :return: Tuple of Weber number, Bond number, and lambda squared.\n",
        "    \"\"\"\n",
        "    velocity_components = [row['ParticleXVelocity_m_s'], row['ParticleYVelocity_m_s'], row['ParticleZVelocity_m_s']]\n",
        "    velocity_magnitude = np.sqrt(sum(v**2 for v in velocity_components))\n",
        "    We = rho_L * velocity_magnitude**2 * row['ParticleDiameter_m'] / gamma\n",
        "    Bo = rho_L * 9.81 * row['ParticleDiameter_m']**2 / gamma\n",
        "    lambda_squared = (row['ParticleDensity_kg_m3'] / rho_L) ** 2\n",
        "    return We, Bo, lambda_squared\n",
        "\n",
        "def determine_state(row, rho_L, gamma):\n",
        "    \"\"\"\n",
        "    Determine the state based on calculated Weber, Bond numbers, and lambda squared.\n",
        "    :param row: Series representing a particle.\n",
        "    :param rho_L: Liquid density.\n",
        "    :param gamma: Surface tension coefficient.\n",
        "    :return: Integer state value.\n",
        "    \"\"\"\n",
        "    We, Bo, lambda_squared = calculate_we_bo_lambda(row, rho_L, gamma)\n",
        "    WeBo = We * np.sqrt(Bo**3)\n",
        "    return 0 if WeBo >= 12 / lambda_squared else 1 if 6 / lambda_squared <= WeBo < 12 / lambda_squared else 2\n",
        "\n",
        "def extract_info_from_filename(filename):\n",
        "    \"\"\"\n",
        "    Extract bar, kgm3, and VM values from the filename. Ignore files not starting with 'file'\n",
        "    and those prefixed with 'processed'.\n",
        "    :param filename: The filename to extract data from.\n",
        "    :return: Tuple of extracted values or (None, None, None) if not valid, processed, or not starting with 'file'.\n",
        "    \"\"\"\n",
        "    # if not filename.startswith('file') or filename.startswith('processed'):\n",
        "    #     return None, None, None\n",
        "\n",
        "    pattern = r'file_(\\d+\\.?\\d*)bar_(\\d+\\.?\\d*)kgm3_(\\d+\\.?\\d*)VM\\.csv'\n",
        "    match = re.search(pattern, filename)\n",
        "    return match.groups() if match else (None, None, None)\n",
        "\n",
        "\n",
        "def process_file(df, rho_L, gamma, start_tracker_id):\n",
        "    \"\"\"\n",
        "    Process each file by determining state, adjusting time, and assigning tracker IDs.\n",
        "    The tracker IDs continue incrementing from the last ID used in the previous file.\n",
        "    :param df: DataFrame of particle data.\n",
        "    :param rho_L: Liquid density.\n",
        "    :param gamma: Surface tension coefficient.\n",
        "    :param start_tracker_id: The starting tracker ID to ensure continuity across files.\n",
        "    :return: DataFrame after processing and the last tracker ID used.\n",
        "    \"\"\"\n",
        "\n",
        "    df['AdjustedTime'] = df.groupby('ParticleID_').apply(\n",
        "        lambda x: x['ParticleResidenceTime_s'] + x.groupby((x['ParticleResidenceTime_s'].diff() > 0).cumsum()).cumcount() * 1e-11\n",
        "    ).reset_index(level=0, drop=True)\n",
        "    processed_df, last_tracker_id = assign_tracker_ids(df, start_tracker_id)\n",
        "    return processed_df, last_tracker_id\n",
        "\n",
        "def assign_tracker_ids(df, start_tracker_id):\n",
        "    \"\"\"\n",
        "    Assign tracker IDs based on Particle ID and Adjusted Time, ensuring global uniqueness.\n",
        "    Tracker IDs increment with each new ParticleID or when the AdjustedTime decreases (reset in time).\n",
        "    :param df: DataFrame with particle data.\n",
        "    :param start_tracker_id: The starting tracker ID to ensure uniqueness.\n",
        "    :return: DataFrame with tracker IDs assigned and the last used tracker_id.\n",
        "    \"\"\"\n",
        "    current_particle_id = None\n",
        "    previous_time = None\n",
        "    tracker_id = start_tracker_id\n",
        "    tracker_ids = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        if row['ParticleID_'] != current_particle_id:\n",
        "            current_particle_id = row['ParticleID_']\n",
        "            tracker_id += 1\n",
        "            previous_time = None\n",
        "        elif previous_time is not None and row['AdjustedTime'] < previous_time:\n",
        "            tracker_id += 1\n",
        "        tracker_ids.append(tracker_id)\n",
        "        previous_time = row['AdjustedTime']\n",
        "\n",
        "    df['tracker_id'] = tracker_ids\n",
        "    return df, tracker_id\n",
        "\n",
        "desired_columns = [\n",
        "    'ParticleID_', 'ParticleXPosition_m', 'ParticleYPosition_m', 'ParticleZPosition_m',\n",
        "    'ParticleXVelocity_m_s', 'ParticleYVelocity_m_s', 'ParticleZVelocity_m_s',\n",
        "    'ParticleDiameter_m', 'ParticleTemperature_K', 'ParticleDensity_kg_m3',\n",
        "    'ParticleMass_kg', 'ParticleRadialPosition_m', 'ParticleThetaPosition',\n",
        "    'ParticleRadialVelocity_m_s', 'ParticleSwirlVelocity_m_s',\n",
        "    'ParticleVelocityMagnitude_m_s', 'ParticleSpecificHeat',\n",
        "    'ParticleBinaryDiffusivity', 'ParticleLawIndex', 'ParticleReynoldsNumber',\n",
        "    'ParticleLiquidVolumeFraction', 'ParticleLiquidMassFraction',\n",
        "    'ParticleVolatileMassFraction', 'ParticleCharMassFraction',\n",
        "    'VaporizationLimitingTime_s', 'ParticleLewisNumber', 'ParticleNusseltNumber',\n",
        "    'BMMassTransferNumber', 'BMHeatTransferNumber', 'state',\n",
        "    'pressure_bar', 'density_kgm3', 'volatile_matter_VM', 'tracker_id'\n",
        "]\n",
        "\n",
        "\n",
        "def process_all_files(folder_path, rho_L, gamma):\n",
        "    \"\"\"\n",
        "    Process all files in the specified folder, ensuring that tracker IDs are unique globally.\n",
        "    :param folder_path: Path to the folder containing the files.\n",
        "    :param rho_L: Liquid density.\n",
        "    :param gamma: Surface tension coefficient.\n",
        "    :return: DataFrame with all processed data.\n",
        "    \"\"\"\n",
        "    all_data = pd.DataFrame()\n",
        "    global_tracker_id = 0  # Initialize global tracker ID\n",
        "    file_list = [f for f in os.listdir(folder_path) if f.endswith('.csv') and f.startswith('file')]\n",
        "    for filename in tqdm(file_list, desc='Processing files'):\n",
        "        pressure, density, volatile_matter = extract_info_from_filename(filename)\n",
        "        if all([pressure, density, volatile_matter]):\n",
        "            df_path = os.path.join(folder_path, filename)\n",
        "            data = pd.read_csv(df_path)\n",
        "            data = sanitize_column_names(data)\n",
        "            processed_data, last_tracker_id = process_file(data, rho_L, gamma, global_tracker_id)\n",
        "            final_data = processed_data.drop_duplicates('tracker_id', keep='last').reset_index(drop=True)\n",
        "            final_data['pressure_bar'] = pressure\n",
        "            final_data['density_kgm3'] = density\n",
        "            final_data['volatile_matter_VM'] = volatile_matter\n",
        "            final_data['state'] = final_data.apply(determine_state, args=(rho_L, gamma), axis=1)\n",
        "            final_data = final_data.loc[:, final_data.columns.intersection(desired_columns)]\n",
        "            all_data = pd.concat([all_data, final_data], ignore_index=True)\n",
        "            global_tracker_id = last_tracker_id\n",
        "        else:\n",
        "            print(f'{filename} is not an expected file name and will not be processed')\n",
        "    return all_data\n",
        "\n",
        "# Process all files and save the aggregated data\n",
        "\n",
        "all_data = process_all_files(folder_path, rho_L, gamma)\n",
        "output_path = os.path.join(folder_path, \"aggregated_processed_data.csv\")\n",
        "all_data.to_csv(output_path, index=False)\n",
        "print(f\"All data processed and saved: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY_VHcuEGRwe"
      },
      "source": [
        "## Interactive Visualization\n",
        "Parallel Coordinates plot to enable the exploration of multi-dimensional data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "aROGTfZ-gzkP",
        "outputId": "cc2cf6d1-db5b-4853-cbfa-0f9fa3552175"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/statcan/furnace/CFD_particle_data/aggregated_processed_data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m all_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/Colab Notebooks/statcan/furnace/CFD_particle_data/aggregated_processed_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Ensure the specified columns are treated as floats\u001b[39;00m\n\u001b[1;32m      7\u001b[0m all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParticleDiameter_m\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParticleDiameter_m\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/statcan/furnace/CFD_particle_data/aggregated_processed_data.csv'"
          ]
        }
      ],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "all_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/statcan/furnace/CFD_particle_data/aggregated_processed_data.csv')\n",
        "\n",
        "# Ensure the specified columns are treated as floats\n",
        "all_data['ParticleDiameter_m'] = all_data['ParticleDiameter_m'].astype(float)\n",
        "all_data['ParticleMass_kg'] = all_data['ParticleMass_kg'].astype(float)\n",
        "all_data['pressure_bar'] = all_data['pressure_bar'].astype(float)\n",
        "all_data['density_kgm3'] = all_data['density_kgm3'].astype(float)\n",
        "all_data['volatile_matter_VM'] = all_data['volatile_matter_VM'].astype(float)\n",
        "all_data['state'] = all_data['state'].astype(str)\n",
        "\n",
        "# Create the Parallel Coordinates plot with custom color mapping for 'state'\n",
        "fig = px.parallel_coordinates(all_data,\n",
        "                              dimensions=[\n",
        "                                  'ParticleDiameter_m',\n",
        "                                  'ParticleMass_kg',\n",
        "                                  'pressure_bar',\n",
        "                                  'density_kgm3',\n",
        "                                  'volatile_matter_VM',\n",
        "                                  'state'\n",
        "                              ],\n",
        "                              labels={\n",
        "                                  'ParticleDiameter_m': 'Particle Diameter (m)',\n",
        "                                  'ParticleMass_kg': 'Particle Mass (kg)',\n",
        "                                  'pressure_bar': 'Pressure (bar)',\n",
        "                                  'density_kgm3': 'Density (kg/m³)',\n",
        "                                  'volatile_matter_VM': 'Velocity Multiplier (VM)',\n",
        "                                  'state': 'State'\n",
        "                              })\n",
        "\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rSIe6nVGcbE"
      },
      "source": [
        "##  Modelisation\n",
        "Optimizing and evaluating different machine learning models to predict the state of particles in CFD simulations. Utilizing a combination of RandomForest, GradientBoosting, and XGBoost classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdQi_QKKk5f6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Remove duplicates from the dataset\n",
        "all_data = all_data[['ParticleDiameter_m', 'ParticleMass_kg', 'pressure_bar', 'density_kgm3', 'volatile_matter_VM', 'state']].drop_duplicates()\n",
        "\n",
        "# Prepare dataset columns\n",
        "X = all_data[['ParticleDiameter_m', 'ParticleMass_kg', 'pressure_bar', 'density_kgm3', 'volatile_matter_VM']]\n",
        "y = all_data['state'].astype(int)  # Ensure y is of integer type\n",
        "\n",
        "# Split the dataset into training, validation, and test sets with stratification\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Define the pipeline and parameter grid for each model\n",
        "models_params = {\n",
        "    'random_forest': {\n",
        "        'pipeline': Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('classifier', RandomForestClassifier(random_state=42))\n",
        "        ]),\n",
        "        'params': {\n",
        "            'classifier__n_estimators': [100, 200],\n",
        "            'classifier__max_depth': [10, 20, None]\n",
        "        }\n",
        "    },\n",
        "    'gradient_boosting': {\n",
        "        'pipeline': Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
        "        ]),\n",
        "        'params': {\n",
        "            'classifier__n_estimators': [100, 200],\n",
        "            'classifier__learning_rate': [0.05, 0.1],\n",
        "            'classifier__max_depth': [3, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    'xgboost': {\n",
        "        'pipeline': Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
        "        ]),\n",
        "        'params': {\n",
        "            'classifier__n_estimators': [100, 200],\n",
        "            'classifier__learning_rate': [0.05, 0.1],\n",
        "            'classifier__max_depth': [3, 5, 7]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize the dictionary to store optimized results\n",
        "optimized_results = {}\n",
        "\n",
        "# Iterate through each model configuration\n",
        "for model_name, model_info in models_params.items():\n",
        "    # Perform grid search using the training data\n",
        "    grid_search = GridSearchCV(model_info['pipeline'], model_info['params'], cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model from grid search\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    test_report = classification_report(y_test, y_test_pred)\n",
        "\n",
        "    # Evaluate the same model on the validation set\n",
        "    y_val_pred = best_model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    val_report = classification_report(y_val, y_val_pred)\n",
        "\n",
        "    # Store results including the validation performance\n",
        "    optimized_results[model_name] = {\n",
        "        'best_model': best_model,\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_classification_report': test_report,\n",
        "        'validation_accuracy': val_accuracy,\n",
        "        'validation_classification_report': val_report\n",
        "    }\n",
        "\n",
        "optimized_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzvmLSisFiQ_"
      },
      "source": [
        "### Classification report of the best performing model trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puIreMyosiLa",
        "outputId": "c0f189aa-042f-4f8b-b2ca-7762fcb62404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xgboost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       989\n",
            "           1       0.71      0.80      0.75        86\n",
            "           2       0.99      1.00      0.99      1953\n",
            "\n",
            "    accuracy                           0.98      3028\n",
            "   macro avg       0.90      0.93      0.91      3028\n",
            "weighted avg       0.99      0.98      0.98      3028\n",
            "\n",
            "State Mapping: {0: 'Penetrating', 1: 'Bouncing', 2: 'Oscillating'}\n"
          ]
        }
      ],
      "source": [
        "#no free lunch theorem\n",
        "# Choose the best model based on accuracy\n",
        "best_model_name = max(optimized_results, key=lambda x: optimized_results[x]['validation_accuracy'])\n",
        "best_model_details = optimized_results[best_model_name]\n",
        "\n",
        "# Save the best model details, including the pipeline\n",
        "filepath = os.path.join(folder_path, \"best_model.pkl\")\n",
        "joblib.dump(best_model_details, filepath)\n",
        "\n",
        "# Print relevant information\n",
        "print(best_model_name)\n",
        "print(best_model_details['validation_classification_report'])\n",
        "print(\"State Mapping: {0: 'Penetrating', 1: 'Bouncing', 2: 'Oscillating'}\")\n",
        "# Load the model details back\n",
        "loaded_model_details = joblib.load(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KSbUOZLFQht"
      },
      "source": [
        "# Predict the fate of the particle based on input data\n",
        "(Assuming the model is already generated in the target folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svytot95-DbI",
        "outputId": "529a10c7-367c-4c4f-b819-004f7243840c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Particle 1 Prediction: 0 (Penetrating)\n",
            "Particle 2 Prediction: 0 (Penetrating)\n",
            "Particle 3 Prediction: 0 (Penetrating)\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/statcan/furnace/CFD_particle_data'\n",
        "filename = \"best_model.pkl\"\n",
        "filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "loaded_model_details = joblib.load(filepath)\n",
        "\n",
        "# Loading the model with the best parameters:\n",
        "loaded_model = loaded_model_details['best_model']\n",
        "# You can now use loaded_model for predictions or further analysis.\n",
        "\n",
        "# Example input values for multiple particles\n",
        "input_data = {\n",
        "    'ParticleDiameter_m': [0.005, 0.007, 0.004],  # Example diameters for three particles\n",
        "    'ParticleMass_kg':    [0.01, 0.02, 0.015],    # Corresponding masses for those particles\n",
        "    'pressure_bar':       [100, 100, 100],\n",
        "    'density_kgm3':       [500, 500, 500],\n",
        "    'volatile_matter_VM': [50, 50, 50],\n",
        "}\n",
        "\n",
        "input_df = pd.DataFrame(input_data)\n",
        "\n",
        "# Assuming 'loaded_model' is your trained model loaded from the previous step\n",
        "predicted_states = loaded_model.predict(input_df)\n",
        "\n",
        "# Dictionary mapping state codes to descriptions\n",
        "state_descriptions = {0: 'Penetrating', 1: 'Bouncing', 2: 'Oscillating'}\n",
        "\n",
        "# Print predictions for each particle\n",
        "for i, state in enumerate(predicted_states):\n",
        "    print(f\"Particle {i+1} Prediction: {state} ({state_descriptions[state]})\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO6ksXSVbbk29GluoHJLdle",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
